2025-02-02 12:36:58.799 | INFO     | __main__:<module>:118 - begin to eval on 4 gpus | tensor parallel size is 1...
2025-02-02 12:36:58.813 | INFO     | modelzipper.tutils:auto_mkdir:343 - ./results/long-context-training-V1-full/global_step100 not exist! --> Create dir ./results/long-context-training-V1-full/global_step100
2025-02-02 12:36:58.814 | INFO     | __main__:<module>:154 - evaluating on datasets: ['hotpotqa_e', 'qasper_e', 'lcc_e', 'repobench-p_e', 'samsum_e', 'musique', 'multifieldqa_en_e', 'gov_report_e', 'passage_retrieval_en_e', 'triviaqa_e', 'multi_news_e', 'passage_count_e', '2wikimqa_e', 'trec_e']
2025-02-02 12:36:58.815 | INFO     | __main__:<module>:156 - max_context_length are set as 32000
  0%|          | 0/4 [00:00<?, ?it/s]2025-02-02 12:37:29.405 | INFO     | __mp_main__:get_pred:25 - gpu id 0 is processing hotpotqa_e length 75 ...
2025-02-02 12:37:34.281 | INFO     | __mp_main__:get_pred:25 - gpu id 1 is processing hotpotqa_e length 75 ...
2025-02-02 12:37:34.662 | INFO     | __mp_main__:get_pred:28 - rank 0 开始加载模型 ...
2025-02-02 12:37:39.559 | INFO     | __mp_main__:get_pred:25 - gpu id 2 is processing hotpotqa_e length 75 ...
2025-02-02 12:37:39.769 | INFO     | __mp_main__:get_pred:28 - rank 1 开始加载模型 ...
2025-02-02 12:37:44.568 | INFO     | __mp_main__:get_pred:25 - gpu id 3 is processing hotpotqa_e length 75 ...
2025-02-02 12:37:55.895 | INFO     | __mp_main__:get_pred:28 - rank 2 开始加载模型 ...
2025-02-02 12:37:58.892 | INFO     | __mp_main__:get_pred:28 - rank 3 开始加载模型 ...
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]slurmstepd: error: *** JOB 5120575 ON SH-IDC1-10-140-24-11 CANCELLED AT 2025-02-02T12:38:33 ***
