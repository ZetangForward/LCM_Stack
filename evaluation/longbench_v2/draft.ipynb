{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# datas = json.load(open(\"/data/pub_data/LongBench-v2/data.json\",\"r\"))\n",
    "from datasets import load_dataset\n",
    "\n",
    "datas = load_dataset(\"/data/pub_data/LongBench-v2/\",split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m x \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m),(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m)]\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(x,):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(k,v)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "x = [(1,2),(3,4)]\n",
    "\n",
    "for k,v in zip(x):\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = [k for k in datas if k[\"length\"]==\"medium\"]\n",
    "# len(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nohup: 忽略输入\n",
      "2025-02-03 19:56:28.877 | INFO     | __main__:<module>:119 - begin to eval on 4 gpus | tensor parallel size is 2...\n",
      "ModelZipper is ready for launch🚀 | Current Version🦄 >>> 0.2.7 <<< | AOE Time🕒 2025-02-03 23:56:24\n",
      "Pid: 254541 Crystalcareai/meta-llama-3.1-8b \n",
      "split_gpu_list: ['4,5', '6,7']\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]2025-02-03 19:56:54.943 | INFO     | __mp_main__:get_pred:48 - gpu id 4,5 is processing Code Repository Understanding length 7 ...\n",
      "2025-02-03 19:56:55.812 | INFO     | __mp_main__:get_pred:51 - rank 4,5 begin to load model ...\n",
      "The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n",
      "ModelZipper is ready for launch🚀 | Current Version🦄 >>> 0.2.7 <<< | AOE Time🕒 2025-02-03 23:56:50\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.83s/it]\n",
      "Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.78s/it]\n",
      "Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.75s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.30s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.47s/it]\n",
      "\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]2025-02-03 19:57:08.571 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 19:57:08.571 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 76493])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "2025-02-03 19:57:12.156 | INFO     | __mp_main__:get_pred:48 - gpu id 6,7 is processing Code Repository Understanding length 8 ...\n",
      "2025-02-03 19:57:12.965 | INFO     | __mp_main__:get_pred:51 - rank 6,7 begin to load model ...\n",
      "The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n",
      "ModelZipper is ready for launch🚀 | Current Version🦄 >>> 0.2.7 <<< | AOE Time🕒 2025-02-03 23:57:07\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.11s/it]\n",
      "Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.94s/it]\n",
      "Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.87s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.36s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.58s/it]\n",
      "\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]2025-02-03 19:57:24.818 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 19:57:24.819 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 48556])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "\n",
      " 14%|█▍        | 1/7 [00:22<02:12, 22.06s/it]2025-02-03 19:57:30.778 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 19:57:30.778 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 114292])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 12%|█▎        | 1/8 [00:13<01:35, 13.67s/it]2025-02-03 19:57:38.648 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 19:57:38.649 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 92753])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 25%|██▌       | 2/8 [00:38<02:02, 20.39s/it]2025-02-03 19:58:03.680 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 19:58:03.681 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 60991])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 29%|██▊       | 2/7 [00:55<02:24, 28.89s/it]2025-02-03 19:58:04.220 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 19:58:04.220 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 30016])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 43%|████▎     | 3/7 [01:00<01:11, 17.77s/it]2025-02-03 19:58:08.748 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 19:58:08.748 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 46641])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 38%|███▊      | 3/8 [00:48<01:17, 15.40s/it]2025-02-03 19:58:13.019 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 19:58:13.019 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 23955])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 57%|█████▋    | 4/7 [01:11<00:45, 15.17s/it]2025-02-03 19:58:19.912 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 19:58:19.913 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 30778])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 50%|█████     | 4/8 [00:55<00:48, 12.19s/it]2025-02-03 19:58:20.471 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 19:58:20.471 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 99336])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 71%|███████▏  | 5/7 [01:19<00:25, 12.73s/it]2025-02-03 19:58:28.425 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 19:58:28.426 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 74392])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 86%|████████▌ | 6/7 [01:38<00:14, 14.88s/it]2025-02-03 19:58:47.486 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 19:58:47.486 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 67354])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 62%|██████▎   | 5/8 [01:23<00:53, 17.79s/it]2025-02-03 19:58:48.272 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 19:58:48.272 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 115032])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      "100%|██████████| 7/7 [01:55<00:00, 15.57s/it]\n",
      "100%|██████████| 7/7 [01:55<00:00, 16.56s/it]\n",
      "\n",
      " 50%|█████     | 1/2 [02:35<02:35, 155.98s/it]\n",
      " 75%|███████▌  | 6/8 [01:57<00:46, 23.32s/it]2025-02-03 19:59:22.270 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 19:59:22.270 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 97163])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 88%|████████▊ | 7/8 [02:24<00:24, 24.60s/it]2025-02-03 19:59:49.498 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 19:59:49.499 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 92607])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      "100%|██████████| 8/8 [02:49<00:00, 24.79s/it]\n",
      "100%|██████████| 8/8 [02:49<00:00, 21.21s/it]\n",
      "\n",
      "100%|██████████| 2/2 [03:46<00:00, 105.70s/it]\n",
      "100%|██████████| 2/2 [03:46<00:00, 113.24s/it]\n",
      "2025-02-03 20:00:16.577 | INFO     | modelzipper.tutils:auto_save_data:296 - ./results/Crystalcareai-meta-llama-3.1-8b not exist! --> Create data dir ./results/Crystalcareai-meta-llama-3.1-8b\n",
      "2025-02-03 20:00:16.580 | INFO     | modelzipper.tutils:auto_save_data:304 - jsonl file saved successfully!\n",
      "2025-02-03 20:00:16.580 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to ./results/Crystalcareai-meta-llama-3.1-8b/Code Repository Understanding.jsonl | len: 15 |  size: 10.58 KB\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]ModelZipper is ready for launch🚀 | Current Version🦄 >>> 0.2.7 <<< | AOE Time🕒 2025-02-03 23:56:37\n",
      "2025-02-03 20:00:42.137 | INFO     | __mp_main__:get_pred:48 - gpu id 4,5 is processing Long In-context Learning length 20 ...\n",
      "2025-02-03 20:00:43.090 | INFO     | __mp_main__:get_pred:51 - rank 4,5 begin to load model ...\n",
      "The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n",
      "ModelZipper is ready for launch🚀 | Current Version🦄 >>> 0.2.7 <<< | AOE Time🕒 2025-02-04 00:00:37\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.87s/it]\n",
      "Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.76s/it]\n",
      "Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.69s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.24s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.42s/it]\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]2025-02-03 20:00:55.929 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:00:55.929 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 94521])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "2025-02-03 20:00:59.498 | INFO     | __mp_main__:get_pred:48 - gpu id 6,7 is processing Long In-context Learning length 20 ...\n",
      "2025-02-03 20:01:00.468 | INFO     | __mp_main__:get_pred:51 - rank 6,7 begin to load model ...\n",
      "The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n",
      "ModelZipper is ready for launch🚀 | Current Version🦄 >>> 0.2.7 <<< | AOE Time🕒 2025-02-04 00:00:54\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.96s/it]\n",
      "Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.92s/it]\n",
      "Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.88s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.35s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.55s/it]\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]2025-02-03 20:01:11.853 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:01:11.853 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 28020])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "\n",
      "  5%|▌         | 1/20 [00:05<01:35,  5.00s/it]2025-02-03 20:01:17.047 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:01:17.047 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 99122])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      "  5%|▌         | 1/20 [00:28<08:59, 28.40s/it]2025-02-03 20:01:24.514 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:01:24.515 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 126858])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 10%|█         | 2/20 [00:24<04:05, 13.62s/it]2025-02-03 20:01:36.579 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:01:36.579 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 33788])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 15%|█▌        | 3/20 [00:28<02:35,  9.15s/it]2025-02-03 20:01:40.507 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:01:40.508 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 90940])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 10%|█         | 2/20 [01:07<10:26, 34.80s/it]2025-02-03 20:02:03.725 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:02:03.726 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 126872])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 20%|██        | 4/20 [00:52<04:03, 15.20s/it]2025-02-03 20:02:05.032 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:02:05.032 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 94872])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 25%|██▌       | 5/20 [01:18<04:46, 19.09s/it]2025-02-03 20:02:31.138 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:02:31.138 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 126856])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 15%|█▌        | 3/20 [01:46<10:26, 36.85s/it]2025-02-03 20:02:42.980 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:02:42.980 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 100627])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 20%|██        | 4/20 [02:06<08:01, 30.07s/it]2025-02-03 20:03:02.622 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:03:02.622 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 80761])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 30%|███       | 6/20 [01:58<06:03, 25.95s/it]2025-02-03 20:03:10.143 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:03:10.143 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 25364])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 35%|███▌      | 7/20 [02:00<03:58, 18.34s/it]2025-02-03 20:03:12.781 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:03:12.781 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 29190])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 40%|████      | 8/20 [02:04<02:42, 13.51s/it]\n",
      " 25%|██▌       | 5/20 [02:20<06:02, 24.19s/it]2025-02-03 20:03:16.070 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:03:16.070 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 78865])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "2025-02-03 20:03:16.518 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:03:16.518 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 126882])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 45%|████▌     | 9/20 [02:17<02:27, 13.41s/it]2025-02-03 20:03:29.310 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:03:29.310 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 86168])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 50%|█████     | 10/20 [02:32<02:19, 13.96s/it]2025-02-03 20:03:44.536 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:03:44.537 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 94809])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 30%|███       | 6/20 [02:59<06:50, 29.35s/it]2025-02-03 20:03:55.697 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:03:55.697 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 78062])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 35%|███▌      | 7/20 [03:13<05:14, 24.20s/it]2025-02-03 20:04:09.483 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:04:09.483 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 124325])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 55%|█████▌    | 11/20 [02:58<02:38, 17.64s/it]2025-02-03 20:04:10.352 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:04:10.352 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 38188])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 60%|██████    | 12/20 [03:02<01:49, 13.65s/it]2025-02-03 20:04:14.994 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:04:14.995 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 94812])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 40%|████      | 8/20 [03:42<05:08, 25.70s/it]2025-02-03 20:04:38.322 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:04:38.322 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 90927])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 65%|██████▌   | 13/20 [03:28<02:01, 17.37s/it]2025-02-03 20:04:41.456 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:04:41.456 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 43285])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 70%|███████   | 14/20 [03:40<01:33, 15.53s/it]2025-02-03 20:04:52.029 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:04:52.030 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 23740])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 45%|████▌     | 9/20 [04:01<04:20, 23.65s/it]2025-02-03 20:04:57.600 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:04:57.601 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 126349])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 75%|███████▌  | 15/20 [03:47<01:05, 13.08s/it]2025-02-03 20:04:59.409 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:04:59.409 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 15632])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 80%|████████  | 16/20 [03:49<00:39,  9.88s/it]2025-02-03 20:05:02.054 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:05:02.054 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 105665])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 50%|█████     | 10/20 [04:31<04:16, 25.61s/it]2025-02-03 20:05:27.164 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:05:27.164 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 12571])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 85%|████████▌ | 17/20 [04:20<00:47, 15.96s/it]2025-02-03 20:05:32.161 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:05:32.162 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 90943])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 55%|█████▌    | 11/20 [04:37<02:56, 19.65s/it]2025-02-03 20:05:33.407 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:05:33.407 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 66061])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 60%|██████    | 12/20 [04:48<02:16, 17.01s/it]2025-02-03 20:05:44.561 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:05:44.561 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 94707])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 90%|█████████ | 18/20 [04:44<00:37, 18.55s/it]2025-02-03 20:05:56.574 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:05:56.575 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 24450])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 95%|█████████▌| 19/20 [04:47<00:13, 13.74s/it]2025-02-03 20:05:59.240 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:05:59.240 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 90930])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 65%|██████▌   | 13/20 [05:14<02:18, 19.75s/it]2025-02-03 20:06:10.525 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:06:10.526 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 90934])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      "100%|██████████| 20/20 [05:09<00:00, 16.31s/it]\n",
      "100%|██████████| 20/20 [05:09<00:00, 15.47s/it]\n",
      "\n",
      " 70%|███████   | 14/20 [05:39<02:07, 21.23s/it]2025-02-03 20:06:35.047 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:06:35.048 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 39769])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 75%|███████▌  | 15/20 [05:46<01:25, 17.07s/it]2025-02-03 20:06:42.526 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:06:42.526 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 59856])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 80%|████████  | 16/20 [06:01<01:05, 16.34s/it]2025-02-03 20:06:57.284 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:06:57.285 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 94689])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 85%|████████▌ | 17/20 [06:27<00:57, 19.25s/it]2025-02-03 20:07:23.130 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:07:23.130 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 40245])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 90%|█████████ | 18/20 [06:32<00:29, 14.92s/it]2025-02-03 20:07:28.091 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:07:28.092 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 90960])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 95%|█████████▌| 19/20 [06:56<00:17, 17.84s/it]2025-02-03 20:07:52.849 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:07:52.849 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 121495])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      "100%|██████████| 20/20 [07:33<00:00, 23.59s/it]\n",
      "100%|██████████| 20/20 [07:33<00:00, 22.69s/it]\n",
      "\n",
      " 50%|█████     | 1/2 [08:14<08:14, 494.54s/it]\n",
      "100%|██████████| 2/2 [08:14<00:00, 247.27s/it]\n",
      "2025-02-03 20:08:31.151 | INFO     | modelzipper.tutils:auto_save_data:304 - jsonl file saved successfully!\n",
      "2025-02-03 20:08:31.151 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to ./results/Crystalcareai-meta-llama-3.1-8b/Long In-context Learning.jsonl | len: 40 |  size: 18.16 KB\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]ModelZipper is ready for launch🚀 | Current Version🦄 >>> 0.2.7 <<< | AOE Time🕒 2025-02-04 00:00:23\n",
      "2025-02-03 20:08:57.061 | INFO     | __mp_main__:get_pred:48 - gpu id 4,5 is processing Long Structured Data Understanding length 3 ...\n",
      "2025-02-03 20:09:07.640 | INFO     | __mp_main__:get_pred:51 - rank 4,5 begin to load model ...\n",
      "2025-02-03 20:09:14.648 | INFO     | __mp_main__:get_pred:48 - gpu id 6,7 is processing Long Structured Data Understanding length 3 ...\n",
      "2025-02-03 20:09:15.528 | INFO     | __mp_main__:get_pred:51 - rank 6,7 begin to load model ...\n",
      "The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n",
      "The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n",
      "ModelZipper is ready for launch🚀 | Current Version🦄 >>> 0.2.7 <<< | AOE Time🕒 2025-02-04 00:09:09\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]ModelZipper is ready for launch🚀 | Current Version🦄 >>> 0.2.7 <<< | AOE Time🕒 2025-02-04 00:08:52\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.86s/it]\n",
      "Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.57s/it]\n",
      "Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.80s/it]\n",
      "Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.56s/it]\n",
      "Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.75s/it]\n",
      "Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.54s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.25s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.45s/it]\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.13s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.29s/it]\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]2025-02-03 20:09:27.715 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:09:27.715 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 123665])\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]2025-02-03 20:09:27.999 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:09:27.999 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 65600])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "\n",
      " 33%|███▎      | 1/3 [00:18<00:37, 18.77s/it]2025-02-03 20:09:46.751 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:09:46.751 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 45734])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 67%|██████▋   | 2/3 [00:30<00:14, 14.50s/it]2025-02-03 20:09:58.159 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:09:58.159 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 16588])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      "100%|██████████| 3/3 [00:37<00:00, 11.01s/it]\n",
      "100%|██████████| 3/3 [00:37<00:00, 12.38s/it]\n",
      "\n",
      " 50%|█████     | 1/2 [01:35<01:35, 95.45s/it]\n",
      " 33%|███▎      | 1/3 [00:40<01:20, 40.07s/it]2025-02-03 20:10:07.805 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:10:07.806 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 102855])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 67%|██████▋   | 2/3 [01:08<00:33, 33.13s/it]2025-02-03 20:10:36.107 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:10:36.107 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 124165])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      "100%|██████████| 3/3 [01:43<00:00, 34.26s/it]\n",
      "100%|██████████| 3/3 [01:43<00:00, 34.65s/it]\n",
      "\n",
      "100%|██████████| 2/2 [02:41<00:00, 78.29s/it]\n",
      "100%|██████████| 2/2 [02:41<00:00, 80.86s/it]\n",
      "2025-02-03 20:11:12.877 | INFO     | modelzipper.tutils:auto_save_data:304 - jsonl file saved successfully!\n",
      "2025-02-03 20:11:12.878 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to ./results/Crystalcareai-meta-llama-3.1-8b/Long Structured Data Understanding.jsonl | len: 6 |  size: 4.45 KB\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]ModelZipper is ready for launch🚀 | Current Version🦄 >>> 0.2.7 <<< | AOE Time🕒 2025-02-04 00:08:38\n",
      "2025-02-03 20:11:37.497 | INFO     | __mp_main__:get_pred:48 - gpu id 4,5 is processing Long-dialogue History Understanding length 10 ...\n",
      "2025-02-03 20:11:38.256 | INFO     | __mp_main__:get_pred:51 - rank 4,5 begin to load model ...\n",
      "The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n",
      "ModelZipper is ready for launch🚀 | Current Version🦄 >>> 0.2.7 <<< | AOE Time🕒 2025-02-04 00:11:32\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.52s/it]\n",
      "Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.54s/it]\n",
      "Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.54s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.12s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.27s/it]\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]2025-02-03 20:11:50.229 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:11:50.229 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 42214])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "2025-02-03 20:11:54.608 | INFO     | __mp_main__:get_pred:48 - gpu id 6,7 is processing Long-dialogue History Understanding length 10 ...\n",
      "2025-02-03 20:11:55.462 | INFO     | __mp_main__:get_pred:51 - rank 6,7 begin to load model ...\n",
      "The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n",
      "\n",
      " 10%|█         | 1/10 [00:09<01:24,  9.38s/it]2025-02-03 20:11:59.566 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:11:59.566 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 23140])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "ModelZipper is ready for launch🚀 | Current Version🦄 >>> 0.2.7 <<< | AOE Time🕒 2025-02-04 00:11:49\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.02s/it]\n",
      "Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.95s/it]\n",
      "Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.89s/it]\n",
      " 20%|██        | 2/10 [00:16<01:04,  8.08s/it]2025-02-03 20:12:06.730 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:12:06.731 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 25171])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.35s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.56s/it]\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]2025-02-03 20:12:07.276 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:12:07.276 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 23220])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "\n",
      " 30%|███       | 3/10 [00:23<00:54,  7.74s/it]2025-02-03 20:12:14.072 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:12:14.072 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 29841])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 10%|█         | 1/10 [00:09<01:23,  9.26s/it]2025-02-03 20:12:16.555 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:12:16.556 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 29732])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 20%|██        | 2/10 [00:13<00:50,  6.33s/it]2025-02-03 20:12:20.825 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:12:20.826 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 29640])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 40%|████      | 4/10 [00:31<00:47,  7.84s/it]2025-02-03 20:12:22.089 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:12:22.089 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 38928])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 30%|███       | 3/10 [00:18<00:39,  5.62s/it]2025-02-03 20:12:25.606 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:12:25.607 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 29696])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 50%|█████     | 5/10 [00:37<00:35,  7.13s/it]2025-02-03 20:12:27.949 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:12:27.949 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 38916])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 40%|████      | 4/10 [00:22<00:31,  5.17s/it]2025-02-03 20:12:30.101 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:12:30.101 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 38916])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 60%|██████    | 6/10 [00:43<00:26,  6.71s/it]2025-02-03 20:12:33.832 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:12:33.832 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 23039])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 50%|█████     | 5/10 [00:28<00:27,  5.47s/it]2025-02-03 20:12:36.074 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:12:36.075 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 25172])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 70%|███████   | 7/10 [00:50<00:20,  6.80s/it]2025-02-03 20:12:40.843 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:12:40.843 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 33858])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 60%|██████    | 6/10 [00:36<00:24,  6.14s/it]2025-02-03 20:12:43.509 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:12:43.510 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 25182])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 80%|████████  | 8/10 [00:55<00:12,  6.27s/it]2025-02-03 20:12:45.929 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:12:45.929 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 23127])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 70%|███████   | 7/10 [00:40<00:16,  5.54s/it]2025-02-03 20:12:47.840 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:12:47.840 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 38008])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 90%|█████████ | 9/10 [01:03<00:06,  6.58s/it]2025-02-03 20:12:53.190 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:12:53.191 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 25302])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 80%|████████  | 8/10 [00:49<00:13,  6.74s/it]2025-02-03 20:12:57.199 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:12:57.199 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 59926])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      "100%|██████████| 10/10 [01:10<00:00,  6.85s/it]\n",
      "100%|██████████| 10/10 [01:10<00:00,  7.05s/it]\n",
      "\n",
      " 50%|█████     | 1/2 [01:49<01:49, 109.63s/it]\n",
      " 90%|█████████ | 9/10 [01:00<00:07,  7.87s/it]2025-02-03 20:13:07.515 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:13:07.516 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 38839])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      "100%|██████████| 10/10 [01:06<00:00,  7.27s/it]\n",
      "100%|██████████| 10/10 [01:06<00:00,  6.61s/it]\n",
      "\n",
      "100%|██████████| 2/2 [02:01<00:00, 52.37s/it] \n",
      "100%|██████████| 2/2 [02:01<00:00, 60.96s/it]\n",
      "2025-02-03 20:13:14.809 | INFO     | modelzipper.tutils:auto_save_data:304 - jsonl file saved successfully!\n",
      "2025-02-03 20:13:14.810 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to ./results/Crystalcareai-meta-llama-3.1-8b/Long-dialogue History Understanding.jsonl | len: 20 |  size: 9.49 KB\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]ModelZipper is ready for launch🚀 | Current Version🦄 >>> 0.2.7 <<< | AOE Time🕒 2025-02-04 00:11:20\n",
      "2025-02-03 20:13:40.065 | INFO     | __mp_main__:get_pred:48 - gpu id 4,5 is processing Multi-Document QA length 43 ...\n",
      "2025-02-03 20:13:40.876 | INFO     | __mp_main__:get_pred:51 - rank 4,5 begin to load model ...\n",
      "The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n",
      "ModelZipper is ready for launch🚀 | Current Version🦄 >>> 0.2.7 <<< | AOE Time🕒 2025-02-04 00:13:35\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.64s/it]\n",
      "Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.62s/it]\n",
      "Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.60s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.33s/it]\n",
      "\n",
      "  0%|          | 0/43 [00:00<?, ?it/s]2025-02-03 20:13:53.381 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:13:53.381 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 64743])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "2025-02-03 20:13:57.602 | INFO     | __mp_main__:get_pred:48 - gpu id 6,7 is processing Multi-Document QA length 43 ...\n",
      "2025-02-03 20:13:58.435 | INFO     | __mp_main__:get_pred:51 - rank 6,7 begin to load model ...\n",
      "The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n",
      "ModelZipper is ready for launch🚀 | Current Version🦄 >>> 0.2.7 <<< | AOE Time🕒 2025-02-04 00:13:52\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.95s/it]\n",
      "Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.92s/it]\n",
      "Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.91s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.37s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.57s/it]\n",
      "\n",
      "  0%|          | 0/43 [00:00<?, ?it/s]2025-02-03 20:14:10.112 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:14:10.112 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 44126])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "\n",
      "  2%|▏         | 1/43 [00:18<13:09, 18.79s/it]2025-02-03 20:14:12.167 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:14:12.168 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 62450])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      "  5%|▍         | 2/43 [00:27<08:57, 13.12s/it]2025-02-03 20:14:21.244 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:14:21.244 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 45416])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      "  2%|▏         | 1/43 [00:12<08:50, 12.64s/it]2025-02-03 20:14:22.800 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:14:22.801 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 55767])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      "  7%|▋         | 3/43 [00:38<08:04, 12.11s/it]2025-02-03 20:14:32.174 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:14:32.175 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 46249])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      "  5%|▍         | 2/43 [00:26<08:58, 13.13s/it]2025-02-03 20:14:36.258 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:14:36.258 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 49500])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      "  9%|▉         | 4/43 [00:45<06:24,  9.87s/it]2025-02-03 20:14:38.540 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:14:38.540 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 22531])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 12%|█▏        | 5/43 [00:52<05:44,  9.06s/it]2025-02-03 20:14:46.164 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:14:46.165 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 29391])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      "  7%|▋         | 3/43 [00:38<08:22, 12.57s/it]2025-02-03 20:14:48.130 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:14:48.131 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 39816])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      "  9%|▉         | 4/43 [00:42<06:10,  9.49s/it]2025-02-03 20:14:53.011 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:14:53.011 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 79922])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 14%|█▍        | 6/43 [01:01<05:23,  8.74s/it]2025-02-03 20:14:54.244 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:14:54.245 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 14761])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 16%|█▋        | 7/43 [01:07<04:46,  7.95s/it]2025-02-03 20:15:00.579 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:15:00.579 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 27163])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 12%|█▏        | 5/43 [00:56<06:54, 10.92s/it]2025-02-03 20:15:06.557 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:15:06.558 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 105692])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 19%|█▊        | 8/43 [01:15<04:36,  7.91s/it]2025-02-03 20:15:08.387 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:15:08.387 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 17033])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 21%|██        | 9/43 [01:21<04:13,  7.47s/it]2025-02-03 20:15:14.862 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:15:14.863 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 11318])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 23%|██▎       | 10/43 [01:27<03:50,  6.99s/it]2025-02-03 20:15:20.786 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:15:20.787 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 15242])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 26%|██▌       | 11/43 [01:29<02:53,  5.42s/it]2025-02-03 20:15:22.708 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:15:22.708 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 25938])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 28%|██▊       | 12/43 [01:36<03:07,  6.05s/it]2025-02-03 20:15:30.174 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:15:30.175 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 23121])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 30%|███       | 13/43 [01:40<02:35,  5.17s/it]2025-02-03 20:15:33.321 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:15:33.322 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 28524])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 33%|███▎      | 14/43 [01:43<02:11,  4.53s/it]\n",
      " 14%|█▍        | 6/43 [01:26<10:46, 17.47s/it]2025-02-03 20:15:36.529 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:15:36.529 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 19624])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "2025-02-03 20:15:36.583 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:15:36.583 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 110041])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 16%|█▋        | 7/43 [01:28<07:26, 12.40s/it]2025-02-03 20:15:38.473 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:15:38.474 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 32557])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 19%|█▊        | 8/43 [01:32<05:41,  9.75s/it]2025-02-03 20:15:42.577 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:15:42.577 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 56169])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 21%|██        | 9/43 [01:40<05:10,  9.13s/it]2025-02-03 20:15:50.391 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:15:50.392 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 57227])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 23%|██▎       | 10/43 [01:54<05:50, 10.62s/it]2025-02-03 20:16:04.241 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:16:04.242 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 10747])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 35%|███▍      | 15/43 [02:15<05:58, 12.79s/it]2025-02-03 20:16:08.444 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:16:08.445 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 50695])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 26%|██▌       | 11/43 [02:00<04:54,  9.20s/it]2025-02-03 20:16:10.191 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:16:10.191 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 11045])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 37%|███▋      | 16/43 [02:21<04:56, 10.99s/it]2025-02-03 20:16:15.204 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:16:15.204 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 57319])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 28%|██▊       | 12/43 [02:06<04:13,  8.19s/it]2025-02-03 20:16:16.193 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:16:16.194 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 39315])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 30%|███       | 13/43 [02:10<03:34,  7.15s/it]2025-02-03 20:16:20.960 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:16:20.961 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 57648])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 40%|███▉      | 17/43 [02:29<04:22, 10.11s/it]2025-02-03 20:16:23.210 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:16:23.210 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 33269])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 42%|████▏     | 18/43 [02:34<03:28,  8.35s/it]2025-02-03 20:16:27.556 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:16:27.557 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 79573])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 33%|███▎      | 14/43 [02:18<03:35,  7.44s/it]2025-02-03 20:16:28.973 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:16:28.973 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 23343])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 35%|███▍      | 15/43 [02:26<03:26,  7.39s/it]2025-02-03 20:16:36.286 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:16:36.287 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 41902])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 44%|████▍     | 19/43 [02:47<03:56,  9.87s/it]2025-02-03 20:16:40.819 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:16:40.819 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 10711])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 37%|███▋      | 16/43 [02:31<03:01,  6.71s/it]2025-02-03 20:16:41.499 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:16:41.499 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 71434])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 47%|████▋     | 20/43 [02:48<02:45,  7.21s/it]2025-02-03 20:16:42.020 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:16:42.020 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 110592])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 40%|███▉      | 17/43 [02:42<03:30,  8.08s/it]2025-02-03 20:16:52.748 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:16:52.748 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 38363])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 42%|████▏     | 18/43 [02:52<03:32,  8.51s/it]2025-02-03 20:17:02.168 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:17:02.169 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 26837])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 49%|████▉     | 21/43 [03:12<04:29, 12.23s/it]2025-02-03 20:17:05.851 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:17:05.851 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 37821])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 44%|████▍     | 19/43 [02:59<03:18,  8.26s/it]2025-02-03 20:17:09.853 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:17:09.853 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 36774])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 51%|█████     | 22/43 [03:17<03:27,  9.90s/it]2025-02-03 20:17:10.480 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:17:10.480 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 113347])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 47%|████▋     | 20/43 [03:08<03:15,  8.51s/it]2025-02-03 20:17:18.888 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:17:18.888 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 11817])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 49%|████▉     | 21/43 [03:09<02:18,  6.27s/it]2025-02-03 20:17:19.945 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:17:19.946 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 17601])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 51%|█████     | 22/43 [03:16<02:12,  6.33s/it]2025-02-03 20:17:26.422 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:17:26.423 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 20174])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 53%|█████▎    | 23/43 [03:22<02:08,  6.41s/it]2025-02-03 20:17:32.984 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:17:32.984 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 10692])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 53%|█████▎    | 23/43 [03:40<04:42, 14.12s/it]2025-02-03 20:17:34.277 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:17:34.277 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 40260])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 56%|█████▌    | 24/43 [03:28<01:57,  6.21s/it]2025-02-03 20:17:38.810 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:17:38.810 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 43736])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 56%|█████▌    | 24/43 [03:50<04:03, 12.80s/it]\n",
      " 58%|█████▊    | 25/43 [03:34<01:47,  5.96s/it]2025-02-03 20:17:44.155 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:17:44.155 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 21815])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "2025-02-03 20:17:44.211 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:17:44.211 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 115917])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 60%|██████    | 26/43 [03:36<01:22,  4.83s/it]2025-02-03 20:17:46.442 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:17:46.442 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 63271])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 63%|██████▎   | 27/43 [03:45<01:38,  6.18s/it]2025-02-03 20:17:55.757 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:17:55.758 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 50632])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 65%|██████▌   | 28/43 [03:52<01:37,  6.51s/it]2025-02-03 20:18:02.939 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:18:02.939 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 15306])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 67%|██████▋   | 29/43 [03:54<01:09,  4.98s/it]2025-02-03 20:18:04.571 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:18:04.571 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 99385])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 58%|█████▊    | 25/43 [04:25<05:47, 19.29s/it]2025-02-03 20:18:18.478 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:18:18.478 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 53278])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 70%|██████▉   | 30/43 [04:13<02:00,  9.24s/it]2025-02-03 20:18:23.670 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:18:23.670 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 65352])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 60%|██████    | 26/43 [04:35<04:43, 16.66s/it]2025-02-03 20:18:28.963 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:18:28.963 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 48978])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 72%|███████▏  | 31/43 [04:23<01:52,  9.41s/it]2025-02-03 20:18:33.349 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:18:33.349 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 12975])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 74%|███████▍  | 32/43 [04:24<01:16,  6.95s/it]2025-02-03 20:18:34.517 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:18:34.518 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 10075])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 63%|██████▎   | 27/43 [04:42<03:41, 13.83s/it]\n",
      " 77%|███████▋  | 33/43 [04:26<00:53,  5.35s/it]2025-02-03 20:18:36.161 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:18:36.161 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 41496])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "2025-02-03 20:18:36.289 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:18:36.289 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 76201])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 65%|██████▌   | 28/43 [04:47<02:48, 11.21s/it]2025-02-03 20:18:41.252 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:18:41.253 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 50541])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 67%|██████▋   | 29/43 [04:54<02:17,  9.85s/it]2025-02-03 20:18:48.019 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:18:48.019 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 73869])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 79%|███████▉  | 34/43 [04:45<01:26,  9.59s/it]2025-02-03 20:18:55.900 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:18:55.900 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 106933])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 70%|██████▉   | 30/43 [05:06<02:16, 10.47s/it]2025-02-03 20:18:59.796 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:18:59.797 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 23810])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 72%|███████▏  | 31/43 [05:13<01:53,  9.50s/it]2025-02-03 20:19:07.071 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:19:07.071 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 51428])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 74%|███████▍  | 32/43 [05:26<01:53, 10.35s/it]2025-02-03 20:19:19.343 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:19:19.343 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 13150])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 77%|███████▋  | 33/43 [05:32<01:30,  9.03s/it]2025-02-03 20:19:25.421 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:19:25.421 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 77845])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 81%|████████▏ | 35/43 [05:16<02:07, 15.92s/it]2025-02-03 20:19:26.450 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:19:26.450 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 49304])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 84%|████████▎ | 36/43 [05:22<01:31, 13.09s/it]2025-02-03 20:19:32.838 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:19:32.839 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 20998])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 79%|███████▉  | 34/43 [05:44<01:31, 10.19s/it]2025-02-03 20:19:38.284 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:19:38.284 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 45878])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 86%|████████▌ | 37/43 [05:29<01:07, 11.24s/it]2025-02-03 20:19:39.885 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:19:39.885 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 71692])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 81%|████████▏ | 35/43 [05:51<01:13,  9.18s/it]2025-02-03 20:19:45.581 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:19:45.582 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 16005])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 88%|████████▊ | 38/43 [05:41<00:57, 11.49s/it]\n",
      " 84%|████████▎ | 36/43 [05:58<00:59,  8.50s/it]2025-02-03 20:19:51.978 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:19:51.978 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 72527])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "2025-02-03 20:19:52.045 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:19:52.046 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 67010])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 86%|████████▌ | 37/43 [06:09<00:55,  9.20s/it]2025-02-03 20:20:02.887 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:20:02.887 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 45285])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 91%|█████████ | 39/43 [05:53<00:46, 11.52s/it]2025-02-03 20:20:03.412 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:20:03.412 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 19012])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 93%|█████████▎| 40/43 [06:00<00:30, 10.06s/it]2025-02-03 20:20:10.145 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:20:10.145 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 49539])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 88%|████████▊ | 38/43 [06:20<00:48,  9.72s/it]2025-02-03 20:20:13.805 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:20:13.806 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 60324])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 95%|█████████▌| 41/43 [06:11<00:21, 10.62s/it]2025-02-03 20:20:22.067 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:20:22.067 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 50362])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 91%|█████████ | 39/43 [06:29<00:37,  9.41s/it]2025-02-03 20:20:22.519 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:20:22.519 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 54881])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 93%|█████████▎| 40/43 [06:37<00:27,  9.01s/it]2025-02-03 20:20:30.483 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:20:30.484 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 25526])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 95%|█████████▌| 41/43 [06:39<00:14,  7.10s/it]2025-02-03 20:20:33.135 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:20:33.135 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 29949])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 98%|█████████▊| 42/43 [06:24<00:11, 11.07s/it]2025-02-03 20:20:34.344 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:20:34.344 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 91417])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 98%|█████████▊| 42/43 [06:43<00:06,  6.17s/it]2025-02-03 20:20:37.065 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:20:37.065 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 10169])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      "100%|██████████| 43/43 [06:45<00:00,  4.72s/it]\n",
      "100%|██████████| 43/43 [06:45<00:00,  9.42s/it]\n",
      "\n",
      " 50%|█████     | 1/2 [07:25<07:25, 445.21s/it]\n",
      "100%|██████████| 43/43 [06:48<00:00, 15.19s/it]\n",
      "100%|██████████| 43/43 [06:48<00:00,  9.51s/it]\n",
      "\n",
      "100%|██████████| 2/2 [07:45<00:00, 195.20s/it]\n",
      "100%|██████████| 2/2 [07:45<00:00, 232.70s/it]\n",
      "2025-02-03 20:21:00.233 | INFO     | modelzipper.tutils:auto_save_data:304 - jsonl file saved successfully!\n",
      "2025-02-03 20:21:00.234 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to ./results/Crystalcareai-meta-llama-3.1-8b/Multi-Document QA.jsonl | len: 86 |  size: 34.93 KB\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]ModelZipper is ready for launch🚀 | Current Version🦄 >>> 0.2.7 <<< | AOE Time🕒 2025-02-04 00:13:22\n",
      "2025-02-03 20:21:25.207 | INFO     | __mp_main__:get_pred:48 - gpu id 4,5 is processing Single-Document QA length 61 ...\n",
      "2025-02-03 20:21:26.324 | INFO     | __mp_main__:get_pred:51 - rank 4,5 begin to load model ...\n",
      "The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n",
      "ModelZipper is ready for launch🚀 | Current Version🦄 >>> 0.2.7 <<< | AOE Time🕒 2025-02-04 00:21:20\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.67s/it]\n",
      "Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.63s/it]\n",
      "Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.60s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.16s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.33s/it]\n",
      "\n",
      "  0%|          | 0/61 [00:00<?, ?it/s]2025-02-03 20:21:39.488 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:21:39.488 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 35648])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "2025-02-03 20:21:42.484 | INFO     | __mp_main__:get_pred:48 - gpu id 6,7 is processing Single-Document QA length 61 ...\n",
      "2025-02-03 20:21:43.528 | INFO     | __mp_main__:get_pred:51 - rank 6,7 begin to load model ...\n",
      "The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n",
      "\n",
      "  2%|▏         | 1/61 [00:07<07:50,  7.84s/it]2025-02-03 20:21:47.297 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:21:47.297 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 23894])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "ModelZipper is ready for launch🚀 | Current Version🦄 >>> 0.2.7 <<< | AOE Time🕒 2025-02-04 00:21:37\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "  3%|▎         | 2/61 [00:10<04:34,  4.66s/it]2025-02-03 20:21:49.908 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:21:49.908 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 85295])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      "Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.94s/it]\n",
      "Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.87s/it]\n",
      "Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.80s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.30s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.50s/it]\n",
      "\n",
      "  0%|          | 0/61 [00:00<?, ?it/s]2025-02-03 20:21:55.430 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:21:55.431 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 16400])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "\n",
      "  2%|▏         | 1/61 [00:08<08:33,  8.56s/it]2025-02-03 20:22:03.997 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:22:03.998 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 19265])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      "  5%|▍         | 3/61 [00:25<09:01,  9.34s/it]2025-02-03 20:22:04.860 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:22:04.861 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 92950])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      "  3%|▎         | 2/61 [00:10<04:32,  4.61s/it]2025-02-03 20:22:05.962 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:22:05.962 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 63861])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      "  5%|▍         | 3/61 [00:19<06:34,  6.80s/it]2025-02-03 20:22:15.283 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:22:15.283 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 25186])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      "  7%|▋         | 4/61 [00:22<04:53,  5.15s/it]2025-02-03 20:22:17.879 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:22:17.879 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 22475])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      "  7%|▋         | 4/61 [00:43<12:07, 12.77s/it]2025-02-03 20:22:22.658 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:22:22.659 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 15716])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      "  8%|▊         | 5/61 [00:29<05:29,  5.89s/it]2025-02-03 20:22:25.091 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:22:25.092 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 23759])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 10%|▉         | 6/61 [00:32<04:18,  4.71s/it]2025-02-03 20:22:27.477 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:22:27.477 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 16693])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      "  8%|▊         | 5/61 [00:49<09:44, 10.43s/it]2025-02-03 20:22:28.926 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:22:28.927 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 23095])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 11%|█▏        | 7/61 [00:38<04:44,  5.27s/it]2025-02-03 20:22:33.908 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:22:33.908 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 14645])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 13%|█▎        | 8/61 [00:39<03:33,  4.03s/it]2025-02-03 20:22:35.286 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:22:35.286 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 17618])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 10%|▉         | 6/61 [00:56<08:31,  9.30s/it]2025-02-03 20:22:36.013 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:22:36.014 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 17605])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 15%|█▍        | 9/61 [00:41<02:51,  3.29s/it]2025-02-03 20:22:36.958 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:22:36.958 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 17527])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 16%|█▋        | 10/61 [00:43<02:32,  3.00s/it]2025-02-03 20:22:39.359 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:22:39.359 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 43266])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 11%|█▏        | 7/61 [01:03<07:32,  8.38s/it]2025-02-03 20:22:42.712 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:22:42.712 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 86295])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 18%|█▊        | 11/61 [00:54<04:23,  5.26s/it]2025-02-03 20:22:49.720 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:22:49.721 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 16595])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 20%|█▉        | 12/61 [00:55<03:23,  4.15s/it]2025-02-03 20:22:51.325 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:22:51.325 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 22675])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 21%|██▏       | 13/61 [01:00<03:26,  4.30s/it]2025-02-03 20:22:56.127 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:22:56.127 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 79100])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 13%|█▎        | 8/61 [01:25<11:29, 13.01s/it]2025-02-03 20:23:05.634 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:23:05.634 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 75266])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 23%|██▎       | 14/61 [01:13<05:28,  7.00s/it]2025-02-03 20:23:09.342 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:23:09.342 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 68376])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 25%|██▍       | 15/61 [01:24<06:10,  8.06s/it]2025-02-03 20:23:19.712 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:23:19.712 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 12822])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 26%|██▌       | 16/61 [01:25<04:36,  6.14s/it]2025-02-03 20:23:21.383 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:23:21.383 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 25034])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 28%|██▊       | 17/61 [01:28<03:42,  5.06s/it]2025-02-03 20:23:23.938 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:23:23.938 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 16417])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 15%|█▍        | 9/61 [01:45<12:59, 14.98s/it]2025-02-03 20:23:24.927 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:23:24.927 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 67591])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 30%|██▉       | 18/61 [01:30<02:52,  4.01s/it]2025-02-03 20:23:25.524 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:23:25.525 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 24847])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 31%|███       | 19/61 [01:32<02:30,  3.58s/it]2025-02-03 20:23:28.064 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:23:28.064 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 17920])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 33%|███▎      | 20/61 [01:39<03:03,  4.47s/it]2025-02-03 20:23:34.626 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:23:34.626 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 20210])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 16%|█▋        | 10/61 [01:55<11:32, 13.57s/it]2025-02-03 20:23:35.192 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:23:35.192 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 22818])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 34%|███▍      | 21/61 [01:41<02:28,  3.72s/it]2025-02-03 20:23:36.615 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:23:36.616 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 18537])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 18%|█▊        | 11/61 [02:02<09:40, 11.62s/it]2025-02-03 20:23:42.438 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:23:42.438 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 43500])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 36%|███▌      | 22/61 [01:47<03:01,  4.65s/it]2025-02-03 20:23:43.485 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:23:43.485 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 33540])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 38%|███▊      | 23/61 [01:52<02:52,  4.53s/it]\n",
      " 20%|█▉        | 12/61 [02:08<07:56,  9.73s/it]2025-02-03 20:23:47.701 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:23:47.701 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 25166])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "2025-02-03 20:23:47.763 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:23:47.763 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 15052])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 39%|███▉      | 24/61 [01:54<02:26,  3.96s/it]2025-02-03 20:23:50.296 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:23:50.296 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 20497])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 41%|████      | 25/61 [01:58<02:18,  3.85s/it]2025-02-03 20:23:53.910 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:23:53.911 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 22514])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 21%|██▏       | 13/61 [02:14<06:56,  8.68s/it]2025-02-03 20:23:54.204 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:23:54.204 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 73060])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 43%|████▎     | 26/61 [02:05<02:50,  4.87s/it]2025-02-03 20:24:01.388 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:24:01.389 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 96436])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 23%|██▎       | 14/61 [02:26<07:31,  9.60s/it]2025-02-03 20:24:05.807 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:24:05.807 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 19737])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 25%|██▍       | 15/61 [02:28<05:36,  7.31s/it]2025-02-03 20:24:07.914 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:24:07.914 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 73992])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 44%|████▍     | 27/61 [02:23<05:02,  8.89s/it]2025-02-03 20:24:19.456 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:24:19.456 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 19096])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 26%|██▌       | 16/61 [02:40<06:31,  8.70s/it]2025-02-03 20:24:19.740 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:24:19.741 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 32884])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 46%|████▌     | 28/61 [02:25<03:44,  6.80s/it]2025-02-03 20:24:21.311 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:24:21.311 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 16614])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 48%|████▊     | 29/61 [02:32<03:34,  6.70s/it]2025-02-03 20:24:27.848 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:24:27.848 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 35226])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 28%|██▊       | 17/61 [02:48<06:21,  8.68s/it]2025-02-03 20:24:28.539 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:24:28.539 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 83799])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 49%|████▉     | 30/61 [02:36<03:02,  5.89s/it]2025-02-03 20:24:31.812 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:24:31.812 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 13858])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 51%|█████     | 31/61 [02:37<02:15,  4.52s/it]2025-02-03 20:24:33.241 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:24:33.242 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 64795])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 30%|██▉       | 18/61 [03:03<07:29, 10.46s/it]2025-02-03 20:24:43.234 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:24:43.235 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 99286])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 52%|█████▏    | 32/61 [02:54<03:53,  8.07s/it]2025-02-03 20:24:49.527 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:24:49.528 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 28763])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 54%|█████▍    | 33/61 [02:58<03:12,  6.87s/it]2025-02-03 20:24:53.796 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:24:53.797 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 101537])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 31%|███       | 19/61 [03:31<10:59, 15.69s/it]2025-02-03 20:25:10.818 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:25:10.818 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 18591])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 56%|█████▌    | 34/61 [03:17<04:51, 10.78s/it]2025-02-03 20:25:13.509 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:25:13.509 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 28609])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 57%|█████▋    | 35/61 [03:21<03:40,  8.48s/it]2025-02-03 20:25:16.736 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:25:16.736 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 86644])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 33%|███▎      | 20/61 [03:38<08:53, 13.01s/it]2025-02-03 20:25:17.529 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:25:17.529 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 16638])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 34%|███▍      | 21/61 [03:44<07:22, 11.07s/it]2025-02-03 20:25:24.082 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:25:24.083 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 14924])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 36%|███▌      | 22/61 [03:46<05:18,  8.17s/it]2025-02-03 20:25:25.788 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:25:25.788 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 96203])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 59%|█████▉    | 36/61 [03:44<05:21, 12.84s/it]2025-02-03 20:25:39.917 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:25:39.918 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 121760])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 38%|███▊      | 23/61 [04:06<07:28, 11.81s/it]2025-02-03 20:25:45.805 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:25:45.805 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 13730])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 39%|███▉      | 24/61 [04:09<05:35,  9.08s/it]2025-02-03 20:25:48.532 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:25:48.533 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 31646])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 41%|████      | 25/61 [04:17<05:19,  8.87s/it]2025-02-03 20:25:57.177 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:25:57.178 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 96173])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 61%|██████    | 37/61 [04:11<06:50, 17.11s/it]2025-02-03 20:26:06.724 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:26:06.724 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 34275])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 62%|██████▏   | 38/61 [04:15<05:02, 13.16s/it]2025-02-03 20:26:10.597 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:26:10.597 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 28097])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 64%|██████▍   | 39/61 [04:22<04:14, 11.56s/it]2025-02-03 20:26:18.746 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:26:18.747 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 99298])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 43%|████▎     | 26/61 [04:44<08:16, 14.19s/it]2025-02-03 20:26:23.829 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:26:23.829 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 99297])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 66%|██████▌   | 40/61 [04:50<05:44, 16.42s/it]2025-02-03 20:26:46.230 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:26:46.230 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 18867])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 67%|██████▋   | 41/61 [04:53<04:03, 12.19s/it]2025-02-03 20:26:48.775 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:26:48.776 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 124240])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 44%|████▍     | 27/61 [05:11<10:21, 18.29s/it]2025-02-03 20:26:51.457 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:26:51.457 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 44026])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 46%|████▌     | 28/61 [05:22<08:48, 16.01s/it]2025-02-03 20:27:02.028 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:27:02.029 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 16857])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 48%|████▊     | 29/61 [05:24<06:18, 11.82s/it]2025-02-03 20:27:04.087 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:27:04.087 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 22847])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 49%|████▉     | 30/61 [05:27<04:44,  9.18s/it]2025-02-03 20:27:07.097 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:27:07.097 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 14752])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 51%|█████     | 31/61 [05:29<03:25,  6.84s/it]2025-02-03 20:27:08.657 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:27:08.658 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 94060])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 69%|██████▉   | 42/61 [05:20<05:20, 16.89s/it]2025-02-03 20:27:16.508 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:27:16.508 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 41811])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 70%|███████   | 43/61 [05:26<04:00, 13.39s/it]2025-02-03 20:27:21.734 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:27:21.734 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 75264])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 52%|█████▏    | 32/61 [05:46<04:50, 10.03s/it]2025-02-03 20:27:25.999 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:27:25.999 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 19013])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 54%|█████▍    | 33/61 [05:48<03:36,  7.74s/it]2025-02-03 20:27:28.341 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:27:28.342 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 16940])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 56%|█████▌    | 34/61 [05:52<02:56,  6.54s/it]2025-02-03 20:27:32.249 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:27:32.250 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 65581])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 72%|███████▏  | 44/61 [05:45<04:17, 15.15s/it]2025-02-03 20:27:40.907 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:27:40.907 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 34718])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 57%|█████▋    | 35/61 [06:02<03:16,  7.55s/it]2025-02-03 20:27:42.249 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:27:42.249 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 93544])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 74%|███████▍  | 45/61 [05:49<03:10, 11.93s/it]2025-02-03 20:27:45.273 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:27:45.273 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 20389])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 75%|███████▌  | 46/61 [05:51<02:14,  8.96s/it]2025-02-03 20:27:47.487 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:27:47.488 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 98129])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 77%|███████▋  | 47/61 [06:10<02:46, 11.91s/it]2025-02-03 20:28:06.075 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:28:06.075 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 15491])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 59%|█████▉    | 36/61 [06:28<05:24, 12.98s/it]2025-02-03 20:28:07.903 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:28:07.903 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 75249])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 79%|███████▊  | 48/61 [06:17<02:13, 10.26s/it]2025-02-03 20:28:12.477 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:28:12.477 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 26705])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 80%|████████  | 49/61 [06:19<01:36,  8.01s/it]2025-02-03 20:28:15.394 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:28:15.395 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 70823])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 82%|████████▏ | 50/61 [06:30<01:38,  8.95s/it]2025-02-03 20:28:26.503 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:28:26.504 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 54498])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 61%|██████    | 37/61 [06:47<05:57, 14.91s/it]2025-02-03 20:28:27.174 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:28:27.174 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 58073])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 84%|████████▎ | 51/61 [06:38<01:25,  8.51s/it]2025-02-03 20:28:33.897 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:28:33.898 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 23351])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 62%|██████▏   | 38/61 [06:55<04:56, 12.91s/it]2025-02-03 20:28:35.675 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:28:35.675 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 124975])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 85%|████████▌ | 52/61 [06:45<01:13,  8.13s/it]2025-02-03 20:28:41.388 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:28:41.388 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 110444])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 64%|██████▍   | 39/61 [07:24<06:26, 17.55s/it]2025-02-03 20:29:04.012 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:29:04.012 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 111361])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 87%|████████▋ | 53/61 [07:17<02:02, 15.33s/it]2025-02-03 20:29:13.323 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:29:13.324 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 32682])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 89%|████████▊ | 54/61 [07:21<01:22, 11.84s/it]2025-02-03 20:29:17.108 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:29:17.109 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 75247])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 90%|█████████ | 55/61 [07:40<01:24, 14.08s/it]\n",
      " 66%|██████▌   | 40/61 [07:57<07:44, 22.12s/it]2025-02-03 20:29:36.496 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:29:36.496 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 96193])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "2025-02-03 20:29:36.796 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:29:36.797 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 104036])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 67%|██████▋   | 41/61 [08:17<07:15, 21.75s/it]2025-02-03 20:29:57.443 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:29:57.443 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 33096])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 69%|██████▉   | 42/61 [08:21<05:10, 16.36s/it]2025-02-03 20:30:01.126 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:30:01.126 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 16848])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 70%|███████   | 43/61 [08:23<03:34, 11.93s/it]\n",
      " 92%|█████████▏| 56/61 [08:07<01:29, 17.83s/it]2025-02-03 20:30:03.068 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:30:03.068 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 125907])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "2025-02-03 20:30:03.089 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:30:03.089 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 98607])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 93%|█████████▎| 57/61 [08:26<01:12, 18.20s/it]2025-02-03 20:30:21.919 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:30:21.919 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 17555])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 95%|█████████▌| 58/61 [08:28<00:39, 13.26s/it]2025-02-03 20:30:23.613 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:30:23.613 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 17278])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 97%|█████████▋| 59/61 [08:34<00:22, 11.22s/it]2025-02-03 20:30:30.071 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:30:30.072 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 15488])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 98%|█████████▊| 60/61 [08:36<00:08,  8.29s/it]2025-02-03 20:30:31.723 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:30:31.723 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 92956])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 72%|███████▏  | 44/61 [09:02<05:40, 20.01s/it]2025-02-03 20:30:41.660 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:30:41.660 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 21268])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 74%|███████▍  | 45/61 [09:04<03:54, 14.66s/it]2025-02-03 20:30:44.091 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:30:44.091 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 113492])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      "100%|██████████| 61/61 [08:54<00:00, 11.18s/it]\n",
      "100%|██████████| 61/61 [08:54<00:00,  8.75s/it]\n",
      "\n",
      " 75%|███████▌  | 46/61 [09:37<05:04, 20.30s/it]2025-02-03 20:31:17.297 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:31:17.297 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 26792])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 77%|███████▋  | 47/61 [09:45<03:50, 16.50s/it]2025-02-03 20:31:24.833 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:31:24.833 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 12666])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 79%|███████▊  | 48/61 [09:46<02:34, 11.89s/it]2025-02-03 20:31:26.303 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:31:26.303 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 106193])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 80%|████████  | 49/61 [10:17<03:29, 17.46s/it]2025-02-03 20:31:56.759 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:31:56.759 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 99298])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 82%|████████▏ | 50/61 [10:44<03:46, 20.57s/it]2025-02-03 20:32:24.521 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:32:24.521 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 96198])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 84%|████████▎ | 51/61 [11:11<03:43, 22.39s/it]2025-02-03 20:32:50.938 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:32:50.938 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 18543])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 85%|████████▌ | 52/61 [11:18<02:39, 17.67s/it]2025-02-03 20:32:57.604 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:32:57.604 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 24135])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 87%|████████▋ | 53/61 [11:25<01:56, 14.57s/it]2025-02-03 20:33:05.083 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:33:05.083 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 80905])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 89%|████████▊ | 54/61 [11:46<01:55, 16.53s/it]2025-02-03 20:33:26.038 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:33:26.038 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 14831])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 90%|█████████ | 55/61 [11:47<01:11, 11.99s/it]2025-02-03 20:33:27.501 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:33:27.501 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 51268])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 92%|█████████▏| 56/61 [12:00<01:00, 12.11s/it]2025-02-03 20:33:40.005 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:33:40.006 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 89888])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 93%|█████████▎| 57/61 [12:16<00:53, 13.37s/it]2025-02-03 20:33:56.175 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:33:56.175 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 29287])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 95%|█████████▌| 58/61 [12:19<00:30, 10.33s/it]2025-02-03 20:33:59.423 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:33:59.423 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 42038])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 97%|█████████▋| 59/61 [12:25<00:17,  8.79s/it]2025-02-03 20:34:04.553 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:34:04.553 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 19863])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      " 98%|█████████▊| 60/61 [12:27<00:06,  6.74s/it]2025-02-03 20:34:06.584 | INFO     | __mp_main__:get_pred:72 - input_shape: \n",
      "2025-02-03 20:34:06.584 | INFO     | __mp_main__:get_pred:73 - torch.Size([1, 48769])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "\n",
      "100%|██████████| 61/61 [12:38<00:00,  8.24s/it]\n",
      "100%|██████████| 61/61 [12:38<00:00, 12.44s/it]\n",
      "\n",
      " 50%|█████     | 1/2 [13:19<13:19, 799.43s/it]\n",
      "100%|██████████| 2/2 [13:19<00:00, 399.72s/it]\n",
      "2025-02-03 20:34:19.678 | INFO     | modelzipper.tutils:auto_save_data:304 - jsonl file saved successfully!\n",
      "2025-02-03 20:34:19.678 | INFO     | modelzipper.tutils:auto_save_data:329 - Save file to ./results/Crystalcareai-meta-llama-3.1-8b/Single-Document QA.jsonl | len: 122 |  size: 45.41 KB\n",
      "2025-02-03 20:34:19.678 | INFO     | __main__:<module>:198 - start to eval ...\n",
      "ModelZipper is ready for launch🚀 | Current Version🦄 >>> 0.2.7 <<< | AOE Time🕒 2025-02-04 00:34:27\n",
      "耗时: 38.001633338133495 分钟\n",
      "命令执行成功！\n",
      "ModelZipper is ready for launch🚀 | Current Version🦄 >>> 0.2.7 <<< | AOE Time🕒 2025-02-04 00:21:07\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"base3_1.log\",\"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[169982,\n",
       " 188667,\n",
       " 192211,\n",
       " 197834,\n",
       " 200223,\n",
       " 209804,\n",
       " 210397,\n",
       " 216214,\n",
       " 222447,\n",
       " 226001,\n",
       " 226874,\n",
       " 227352,\n",
       " 228083,\n",
       " 233722,\n",
       " 236215,\n",
       " 239169,\n",
       " 240672,\n",
       " 241749,\n",
       " 244743,\n",
       " 259071,\n",
       " 261180,\n",
       " 261554,\n",
       " 270679,\n",
       " 275565,\n",
       " 278311,\n",
       " 281470,\n",
       " 282443,\n",
       " 283765,\n",
       " 283823,\n",
       " 283854,\n",
       " 287498,\n",
       " 292355,\n",
       " 294705,\n",
       " 296781,\n",
       " 298444,\n",
       " 303510,\n",
       " 306252,\n",
       " 307844,\n",
       " 309484,\n",
       " 312284,\n",
       " 314738,\n",
       " 316318,\n",
       " 325270,\n",
       " 329047,\n",
       " 329526,\n",
       " 332556,\n",
       " 332570,\n",
       " 332570,\n",
       " 332570,\n",
       " 336462,\n",
       " 336986,\n",
       " 340811,\n",
       " 341229,\n",
       " 345177,\n",
       " 346908,\n",
       " 346908,\n",
       " 346908,\n",
       " 346908,\n",
       " 346908,\n",
       " 346908,\n",
       " 349105,\n",
       " 352693,\n",
       " 366845,\n",
       " 369293,\n",
       " 378895,\n",
       " 382083,\n",
       " 382774,\n",
       " 384187,\n",
       " 391297,\n",
       " 391714,\n",
       " 399444,\n",
       " 399444,\n",
       " 405951,\n",
       " 416747,\n",
       " 418761,\n",
       " 418820,\n",
       " 419455,\n",
       " 425025,\n",
       " 425156,\n",
       " 425156,\n",
       " 425156,\n",
       " 425156,\n",
       " 433523,\n",
       " 433523,\n",
       " 433523,\n",
       " 433523,\n",
       " 433523,\n",
       " 433523,\n",
       " 435687,\n",
       " 435687,\n",
       " 435687,\n",
       " 435687,\n",
       " 438649,\n",
       " 439845,\n",
       " 445748,\n",
       " 449472,\n",
       " 450764,\n",
       " 453895,\n",
       " 455689,\n",
       " 455774,\n",
       " 456984,\n",
       " 458615,\n",
       " 463484,\n",
       " 467976,\n",
       " 468899,\n",
       " 470212,\n",
       " 472100,\n",
       " 472131,\n",
       " 473190,\n",
       " 475079,\n",
       " 479543,\n",
       " 484719,\n",
       " 485212,\n",
       " 485709,\n",
       " 488760,\n",
       " 489539,\n",
       " 492273,\n",
       " 493276,\n",
       " 495898,\n",
       " 496131,\n",
       " 496131,\n",
       " 496131,\n",
       " 496131,\n",
       " 512017,\n",
       " 513588,\n",
       " 514887,\n",
       " 516593,\n",
       " 522120,\n",
       " 522256,\n",
       " 525209,\n",
       " 532956,\n",
       " 534147,\n",
       " 536571,\n",
       " 539008,\n",
       " 543378,\n",
       " 543592,\n",
       " 544722,\n",
       " 546183,\n",
       " 547486,\n",
       " 547763,\n",
       " 548058,\n",
       " 548192,\n",
       " 549043,\n",
       " 549551,\n",
       " 549961,\n",
       " 550357,\n",
       " 550914,\n",
       " 550914,\n",
       " 553979,\n",
       " 554114,\n",
       " 554946,\n",
       " 555114,\n",
       " 555289,\n",
       " 558039,\n",
       " 561248,\n",
       " 564487,\n",
       " 566401,\n",
       " 567733,\n",
       " 570828,\n",
       " 575381,\n",
       " 581261,\n",
       " 583605,\n",
       " 584415,\n",
       " 590572,\n",
       " 593385,\n",
       " 594298,\n",
       " 601885,\n",
       " 606899,\n",
       " 607741,\n",
       " 614136,\n",
       " 614556,\n",
       " 620252,\n",
       " 623006,\n",
       " 627099,\n",
       " 627099,\n",
       " 627099,\n",
       " 627099,\n",
       " 627099,\n",
       " 629200,\n",
       " 634754,\n",
       " 635436,\n",
       " 636046,\n",
       " 641196,\n",
       " 645487,\n",
       " 645893,\n",
       " 654747,\n",
       " 666881,\n",
       " 667717,\n",
       " 694284,\n",
       " 708655,\n",
       " 709353,\n",
       " 714990,\n",
       " 738335,\n",
       " 747958,\n",
       " 758267,\n",
       " 760508,\n",
       " 762530,\n",
       " 762802,\n",
       " 785828,\n",
       " 789435,\n",
       " 798672,\n",
       " 815496,\n",
       " 831879,\n",
       " 837902,\n",
       " 871676,\n",
       " 875897,\n",
       " 915974,\n",
       " 916788,\n",
       " 975217,\n",
       " 975497,\n",
       " 1044274,\n",
       " 1241215,\n",
       " 1275474,\n",
       " 1710418,\n",
       " 2232827]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([len(k[\"context\"]) for k in m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7863136117752796"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2232827/589710"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Single-Document QA.jsonl',\n",
       "       'Code Repository Understanding.jsonl',\n",
       "       'Long-dialogue History Understanding.jsonl',\n",
       "       'Long In-context Learning.jsonl', 'Multi-Document QA.jsonl',\n",
       "       'Long Structured Data Understanding.jsonl', 'Overall'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for path  in [\n",
    "    \"/mnt/petrelfs/tangzecheng/LCM_Stack/evaluation/longbench_v2/results/cd_lm_full-0.005-global_step300/result.csv\",\n",
    "    \"/mnt/petrelfs/tangzecheng/LCM_Stack/evaluation/longbench_v2/results/contextual_denoising/ins_adjust_weight_base_full-0.05-global_step250/result.csv\",\n",
    "    \"/mnt/petrelfs/tangzecheng/LCM_Stack/evaluation/longbench_v2/results/Llama-3.1-8B-Instruct-full/result.csv\",\n",
    "    \"/mnt/petrelfs/tangzecheng/LCM_Stack/evaluation/longbench_v2/results/Llama-3.1-8B-Instruct-lora/result.csv\",\n",
    "    \"/mnt/petrelfs/tangzecheng/LCM_Stack/evaluation/longbench_v2/results/Llama-3.1-8B-perturb_loss/result.csv\",\n",
    "    \"/mnt/petrelfs/tangzecheng/LCM_Stack/evaluation/longbench_v2/results/Llama-3.1-8B-perturb_loss_3/result.csv\",\n",
    "    \"/mnt/petrelfs/tangzecheng/LCM_Stack/evaluation/longbench_v2/results/Llama-3.1-8B-real_opposite_gradient_large_pos-1e-3-v2/result.csv\",\n",
    "    \"/mnt/petrelfs/tangzecheng/LCM_Stack/evaluation/longbench_v2/results/llama3.1-8B-pg19-longce-200ep/result.csv\",\n",
    "    \"/mnt/petrelfs/tangzecheng/LCM_Stack/evaluation/longbench_v2/results/Llama3.1-sft-longbench/vanilla/result.csv\",\n",
    "    \"/mnt/petrelfs/tangzecheng/LCM_Stack/evaluation/longbench_v2/results/meta-llama-3.1-8b/vanilla/result.csv\",\n",
    "    \"/mnt/petrelfs/tangzecheng/LCM_Stack/evaluation/longbench_v2/results/Meta-Llama-3.1-8B-Instruct/vanilla/result.csv\"\n",
    "]:\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "    df.columns\n",
    "\n",
    "    columns = ['Code Repository Understanding.jsonl',\n",
    "    'Long In-context Learning.jsonl',\n",
    "    'Long Structured Data Understanding.jsonl',\n",
    "    'Long-dialogue History Understanding.jsonl',\n",
    "    'Multi-Document QA.jsonl',\n",
    "    'Single-Document QA.jsonl',\n",
    "    'Overall']\n",
    "\n",
    "    df = df[columns]\n",
    "    df.to_csv(path.replace(\".csv\",\"n.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(503,\n",
       " dict_keys(['_id', 'domain', 'sub_domain', 'difficulty', 'length', 'question', 'choice_A', 'choice_B', 'choice_C', 'choice_D', 'answer', 'context']))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datas),datas[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/395 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (135552 > 131072). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 395/395 [01:18<00:00,  5.04it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "datas = [k for k in datas if k[\"length\"]!=\"long\"]\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/data/hf_models/Meta-Llama-3.1-8B-Instruct/\")\n",
    "\n",
    "lst = []\n",
    "for d in tqdm(datas):\n",
    "    lst.append(tokenizer(d[\"context\"],return_tensors='pt').input_ids.shape[1])\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'long'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datas[0]['length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/108 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (274575 > 131072). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 108/108 [03:47<00:00,  2.11s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "datas = [k for k in datas if k[\"length\"]== \"long\"]\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/data/hf_models/Meta-Llama-3.1-8B-Instruct/\")\n",
    "\n",
    "lst2 = []\n",
    "for d in tqdm(datas):\n",
    "    lst.append(tokenizer(d[\"context\"],return_tensors='pt').input_ids.shape[1])\n",
    "    # break\n",
    "# len(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lst2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "369"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9913,\n",
       " 10048,\n",
       " 10554,\n",
       " 10557,\n",
       " 10588,\n",
       " 10905,\n",
       " 11224,\n",
       " 11644,\n",
       " 12433,\n",
       " 12450,\n",
       " 12525,\n",
       " 12753,\n",
       " 12895,\n",
       " 13473,\n",
       " 13690,\n",
       " 14414,\n",
       " 14614,\n",
       " 14617,\n",
       " 14670,\n",
       " 14679,\n",
       " 14729,\n",
       " 15040,\n",
       " 15094,\n",
       " 15316,\n",
       " 15399,\n",
       " 15411,\n",
       " 15457,\n",
       " 15912,\n",
       " 16182,\n",
       " 16223,\n",
       " 16369,\n",
       " 16389,\n",
       " 16459,\n",
       " 16485,\n",
       " 16549,\n",
       " 16599,\n",
       " 16667,\n",
       " 16784,\n",
       " 16877,\n",
       " 17172,\n",
       " 17283,\n",
       " 17412,\n",
       " 17431,\n",
       " 17454,\n",
       " 17494,\n",
       " 17726,\n",
       " 18361,\n",
       " 18408,\n",
       " 18413,\n",
       " 18628,\n",
       " 18662,\n",
       " 18807,\n",
       " 18858,\n",
       " 19072,\n",
       " 19419,\n",
       " 19426,\n",
       " 19625,\n",
       " 19729,\n",
       " 19908,\n",
       " 20082,\n",
       " 20234,\n",
       " 20617,\n",
       " 21133,\n",
       " 21364,\n",
       " 22205,\n",
       " 22312,\n",
       " 22427,\n",
       " 22456,\n",
       " 22567,\n",
       " 22602,\n",
       " 22784,\n",
       " 22927,\n",
       " 22952,\n",
       " 23049,\n",
       " 23062,\n",
       " 23142,\n",
       " 23174,\n",
       " 23181,\n",
       " 23443,\n",
       " 23587,\n",
       " 23634,\n",
       " 23660,\n",
       " 23719,\n",
       " 23989,\n",
       " 24329,\n",
       " 24595,\n",
       " 24734,\n",
       " 24869,\n",
       " 25042,\n",
       " 25077,\n",
       " 25094,\n",
       " 25094,\n",
       " 25104,\n",
       " 25225,\n",
       " 25375,\n",
       " 25833,\n",
       " 26081,\n",
       " 26667,\n",
       " 26710,\n",
       " 27007,\n",
       " 27851,\n",
       " 27871,\n",
       " 27887,\n",
       " 28351,\n",
       " 28494,\n",
       " 28764,\n",
       " 29145,\n",
       " 29154,\n",
       " 29546,\n",
       " 29605,\n",
       " 29619,\n",
       " 29639,\n",
       " 29748,\n",
       " 29864,\n",
       " 30560,\n",
       " 31530,\n",
       " 31800,\n",
       " 32405,\n",
       " 32634,\n",
       " 32805,\n",
       " 33065,\n",
       " 33373,\n",
       " 33447,\n",
       " 33780,\n",
       " 34023,\n",
       " 34650,\n",
       " 35101,\n",
       " 35511,\n",
       " 36388,\n",
       " 37442,\n",
       " 37771,\n",
       " 37931,\n",
       " 38224,\n",
       " 38727,\n",
       " 38762,\n",
       " 38839,\n",
       " 38839,\n",
       " 38850,\n",
       " 39495,\n",
       " 39590,\n",
       " 39885,\n",
       " 40041,\n",
       " 40792,\n",
       " 41513,\n",
       " 41690,\n",
       " 41940,\n",
       " 42137,\n",
       " 43076,\n",
       " 43149,\n",
       " 43261,\n",
       " 43398,\n",
       " 43816,\n",
       " 43995,\n",
       " 44996,\n",
       " 45221,\n",
       " 45540,\n",
       " 45628,\n",
       " 45888,\n",
       " 46220,\n",
       " 48385,\n",
       " 48634,\n",
       " 48726,\n",
       " 48817,\n",
       " 49352,\n",
       " 49444,\n",
       " 50189,\n",
       " 50283,\n",
       " 50433,\n",
       " 50461,\n",
       " 51101,\n",
       " 51234,\n",
       " 53178,\n",
       " 54388,\n",
       " 54585,\n",
       " 55607,\n",
       " 55987,\n",
       " 56938,\n",
       " 57175,\n",
       " 57413,\n",
       " 57865,\n",
       " 59385,\n",
       " 59614,\n",
       " 59849,\n",
       " 60883,\n",
       " 62343,\n",
       " 62933,\n",
       " 63625,\n",
       " 64607,\n",
       " 64645,\n",
       " 65215,\n",
       " 65339,\n",
       " 65354,\n",
       " 65745,\n",
       " 66742,\n",
       " 66993,\n",
       " 67337,\n",
       " 68095,\n",
       " 70674,\n",
       " 71041,\n",
       " 71534,\n",
       " 72028,\n",
       " 72612,\n",
       " 73629,\n",
       " 73688,\n",
       " 74269,\n",
       " 75141,\n",
       " 75147,\n",
       " 75147,\n",
       " 75147,\n",
       " 76114,\n",
       " 76330,\n",
       " 77552,\n",
       " 77839,\n",
       " 78766,\n",
       " 78991,\n",
       " 79448,\n",
       " 79570,\n",
       " 80549,\n",
       " 80734,\n",
       " 83645,\n",
       " 85203,\n",
       " 85782,\n",
       " 85888,\n",
       " 86405,\n",
       " 89813,\n",
       " 90832,\n",
       " 90832,\n",
       " 90832,\n",
       " 90832,\n",
       " 90832,\n",
       " 90832,\n",
       " 91314,\n",
       " 92144,\n",
       " 92646,\n",
       " 92854,\n",
       " 92854,\n",
       " 93400,\n",
       " 93829,\n",
       " 94282,\n",
       " 94282,\n",
       " 94282,\n",
       " 94282,\n",
       " 94282,\n",
       " 94282,\n",
       " 96069,\n",
       " 96069,\n",
       " 96069,\n",
       " 96069,\n",
       " 96291,\n",
       " 96853,\n",
       " 97888,\n",
       " 98456,\n",
       " 98920,\n",
       " 98971,\n",
       " 99195,\n",
       " 99195,\n",
       " 99195,\n",
       " 99195,\n",
       " 99209,\n",
       " 100330,\n",
       " 101384,\n",
       " 102734,\n",
       " 103877,\n",
       " 105453,\n",
       " 105477,\n",
       " 106088,\n",
       " 106690,\n",
       " 108649,\n",
       " 109852,\n",
       " 110193,\n",
       " 110283,\n",
       " 111131,\n",
       " 113215,\n",
       " 113248,\n",
       " 113830,\n",
       " 114844,\n",
       " 115576,\n",
       " 115772,\n",
       " 115796,\n",
       " 115820,\n",
       " 116023,\n",
       " 116158,\n",
       " 116866,\n",
       " 117453,\n",
       " 117713,\n",
       " 117718,\n",
       " 117853,\n",
       " 117960,\n",
       " 118137,\n",
       " 118360,\n",
       " 118511,\n",
       " 118604,\n",
       " 118624,\n",
       " 118943,\n",
       " 119149,\n",
       " 119299,\n",
       " 119355,\n",
       " 121281,\n",
       " 121535,\n",
       " 122386,\n",
       " 123556,\n",
       " 123898,\n",
       " 124012,\n",
       " 124143,\n",
       " 124734,\n",
       " 125687,\n",
       " 126183,\n",
       " 126750,\n",
       " 126750,\n",
       " 126750,\n",
       " 126750,\n",
       " 128187,\n",
       " 128663,\n",
       " 129351,\n",
       " 129404,\n",
       " 129677,\n",
       " 129726,\n",
       " 130101,\n",
       " 130523,\n",
       " 130563,\n",
       " 130750,\n",
       " 133076,\n",
       " 134057,\n",
       " 135552,\n",
       " 137172,\n",
       " 138692,\n",
       " 138707,\n",
       " 140363,\n",
       " 141130,\n",
       " 141160,\n",
       " 142067,\n",
       " 143885,\n",
       " 145108,\n",
       " 145323,\n",
       " 145618,\n",
       " 146874,\n",
       " 146881,\n",
       " 147446,\n",
       " 147446,\n",
       " 147446,\n",
       " 147446,\n",
       " 147446,\n",
       " 147741,\n",
       " 148149,\n",
       " 148619,\n",
       " 149124,\n",
       " 153312,\n",
       " 155197,\n",
       " 158160,\n",
       " 159069,\n",
       " 160486,\n",
       " 160603,\n",
       " 160817,\n",
       " 161067,\n",
       " 165678,\n",
       " 168551,\n",
       " 168913,\n",
       " 169964,\n",
       " 170456,\n",
       " 171335,\n",
       " 171360,\n",
       " 171781,\n",
       " 171781,\n",
       " 175246,\n",
       " 176338,\n",
       " 176389,\n",
       " 176773,\n",
       " 176862,\n",
       " 177400,\n",
       " 181467,\n",
       " 185448,\n",
       " 186987,\n",
       " 192393,\n",
       " 194666,\n",
       " 197223,\n",
       " 199439,\n",
       " 200364,\n",
       " 201454,\n",
       " 209122,\n",
       " 209616,\n",
       " 214333,\n",
       " 215060,\n",
       " 223946,\n",
       " 248975,\n",
       " 250744,\n",
       " 280384,\n",
       " 301797,\n",
       " 322935,\n",
       " 352781,\n",
       " 365673,\n",
       " 411830,\n",
       " 411962,\n",
       " 413191,\n",
       " 427447,\n",
       " 589710]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 7, 8, 9]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in range(10) if i >5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m      3\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhleqwr (A\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m(([ABCD])\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pred = \"hleqwr (A\"\n",
    "re.search(\"\\(([ABCD])\\)\",pred).group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "311"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst=[9913,\n",
    " 10048,\n",
    " 10554,\n",
    " 10557,\n",
    " 10588,\n",
    " 10905,\n",
    " 11224,\n",
    " 11644,\n",
    " 12433,\n",
    " 12450,\n",
    " 12525,\n",
    " 12753,\n",
    " 12895,\n",
    " 13473,\n",
    " 13690,\n",
    " 14414,\n",
    " 14614,\n",
    " 14617,\n",
    " 14670,\n",
    " 14679,\n",
    " 14729,\n",
    " 15040,\n",
    " 15094,\n",
    " 15316,\n",
    " 15399,\n",
    " 15411,\n",
    " 15457,\n",
    " 15912,\n",
    " 16182,\n",
    " 16223,\n",
    " 16369,\n",
    " 16389,\n",
    " 16459,\n",
    " 16485,\n",
    " 16549,\n",
    " 16599,\n",
    " 16667,\n",
    " 16784,\n",
    " 16877,\n",
    " 17172,\n",
    " 17283,\n",
    " 17412,\n",
    " 17431,\n",
    " 17454,\n",
    " 17494,\n",
    " 17726,\n",
    " 18361,\n",
    " 18408,\n",
    " 18413,\n",
    " 18628,\n",
    " 18662,\n",
    " 18807,\n",
    " 18858,\n",
    " 19072,\n",
    " 19419,\n",
    " 19426,\n",
    " 19625,\n",
    " 19729,\n",
    " 19908,\n",
    " 20082,\n",
    " 20234,\n",
    " 20617,\n",
    " 21133,\n",
    " 21364,\n",
    " 22205,\n",
    " 22312,\n",
    " 22427,\n",
    " 22456,\n",
    " 22567,\n",
    " 22602,\n",
    " 22784,\n",
    " 22927,\n",
    " 22952,\n",
    " 23049,\n",
    " 23062,\n",
    " 23142,\n",
    " 23174,\n",
    " 23181,\n",
    " 23443,\n",
    " 23587,\n",
    " 23634,\n",
    " 23660,\n",
    " 23719,\n",
    " 23989,\n",
    " 24329,\n",
    " 24595,\n",
    " 24734,\n",
    " 24869,\n",
    " 25042,\n",
    " 25077,\n",
    " 25094,\n",
    " 25094,\n",
    " 25104,\n",
    " 25225,\n",
    " 25375,\n",
    " 25833,\n",
    " 26081,\n",
    " 26667,\n",
    " 26710,\n",
    " 27007,\n",
    " 27851,\n",
    " 27871,\n",
    " 27887,\n",
    " 28351,\n",
    " 28494,\n",
    " 28764,\n",
    " 29145,\n",
    " 29154,\n",
    " 29546,\n",
    " 29605,\n",
    " 29619,\n",
    " 29639,\n",
    " 29748,\n",
    " 29864,\n",
    " 30560,\n",
    " 31530,\n",
    " 31800,\n",
    " 32405,\n",
    " 32634,\n",
    " 32805,\n",
    " 33065,\n",
    " 33373,\n",
    " 33447,\n",
    " 33780,\n",
    " 34023,\n",
    " 34650,\n",
    " 35101,\n",
    " 35511,\n",
    " 36388,\n",
    " 37442,\n",
    " 37771,\n",
    " 37931,\n",
    " 38224,\n",
    " 38727,\n",
    " 38762,\n",
    " 38839,\n",
    " 38839,\n",
    " 38850,\n",
    " 39495,\n",
    " 39590,\n",
    " 39885,\n",
    " 40041,\n",
    " 40792,\n",
    " 41513,\n",
    " 41690,\n",
    " 41940,\n",
    " 42137,\n",
    " 43076,\n",
    " 43149,\n",
    " 43261,\n",
    " 43398,\n",
    " 43816,\n",
    " 43995,\n",
    " 44996,\n",
    " 45221,\n",
    " 45540,\n",
    " 45628,\n",
    " 45888,\n",
    " 46220,\n",
    " 48385,\n",
    " 48634,\n",
    " 48726,\n",
    " 48817,\n",
    " 49352,\n",
    " 49444,\n",
    " 50189,\n",
    " 50283,\n",
    " 50433,\n",
    " 50461,\n",
    " 51101,\n",
    " 51234,\n",
    " 53178,\n",
    " 54388,\n",
    " 54585,\n",
    " 55607,\n",
    " 55987,\n",
    " 56938,\n",
    " 57175,\n",
    " 57413,\n",
    " 57865,\n",
    " 59385,\n",
    " 59614,\n",
    " 59849,\n",
    " 60883,\n",
    " 62343,\n",
    " 62933,\n",
    " 63625,\n",
    " 64607,\n",
    " 64645,\n",
    " 65215,\n",
    " 65339,\n",
    " 65354,\n",
    " 65745,\n",
    " 66742,\n",
    " 66993,\n",
    " 67337,\n",
    " 68095,\n",
    " 70674,\n",
    " 71041,\n",
    " 71534,\n",
    " 72028,\n",
    " 72612,\n",
    " 73629,\n",
    " 73688,\n",
    " 74269,\n",
    " 75141,\n",
    " 75147,\n",
    " 75147,\n",
    " 75147,\n",
    " 76114,\n",
    " 76330,\n",
    " 77552,\n",
    " 77839,\n",
    " 78766,\n",
    " 78991,\n",
    " 79448,\n",
    " 79570,\n",
    " 80549,\n",
    " 80734,\n",
    " 83645,\n",
    " 85203,\n",
    " 85782,\n",
    " 85888,\n",
    " 86405,\n",
    " 89813,\n",
    " 90832,\n",
    " 90832,\n",
    " 90832,\n",
    " 90832,\n",
    " 90832,\n",
    " 90832,\n",
    " 91314,\n",
    " 92144,\n",
    " 92646,\n",
    " 92854,\n",
    " 92854,\n",
    " 93400,\n",
    " 93829,\n",
    " 94282,\n",
    " 94282,\n",
    " 94282,\n",
    " 94282,\n",
    " 94282,\n",
    " 94282,\n",
    " 96069,\n",
    " 96069,\n",
    " 96069,\n",
    " 96069,\n",
    " 96291,\n",
    " 96853,\n",
    " 97888,\n",
    " 98456,\n",
    " 98920,\n",
    " 98971,\n",
    " 99195,\n",
    " 99195,\n",
    " 99195,\n",
    " 99195,\n",
    " 99209,\n",
    " 100330,\n",
    " 101384,\n",
    " 102734,\n",
    " 103877,\n",
    " 105453,\n",
    " 105477,\n",
    " 106088,\n",
    " 106690,\n",
    " 108649,\n",
    " 109852,\n",
    " 110193,\n",
    " 110283,\n",
    " 111131,\n",
    " 113215,\n",
    " 113248,\n",
    " 113830,\n",
    " 114844,\n",
    " 115576,\n",
    " 115772,\n",
    " 115796,\n",
    " 115820,\n",
    " 116023,\n",
    " 116158,\n",
    " 116866,\n",
    " 117453,\n",
    " 117713,\n",
    " 117718,\n",
    " 117853,\n",
    " 117960,\n",
    " 118137,\n",
    " 118360,\n",
    " 118511,\n",
    " 118604,\n",
    " 118624,\n",
    " 118943,\n",
    " 119149,\n",
    " 119299,\n",
    " 119355,\n",
    " 121281,\n",
    " 121535,\n",
    " 122386,\n",
    " 123556,\n",
    " 123898,\n",
    " 124012,\n",
    " 124143,\n",
    " 124734,\n",
    " 125687,\n",
    " 126183,\n",
    " 126750,\n",
    " 126750,\n",
    " 126750,\n",
    " 126750,\n",
    " 128187,\n",
    " 128663,\n",
    " 129351,\n",
    " 129404,\n",
    " 129677,\n",
    " 129726,\n",
    " 130101,\n",
    " 130523,\n",
    " 130563,\n",
    " 130750,\n",
    " 133076,\n",
    " 134057,\n",
    " 135552,\n",
    " 137172,\n",
    " 138692,\n",
    " 138707,\n",
    " 140363,\n",
    " 141130,\n",
    " 141160,\n",
    " 142067,\n",
    " 143885,\n",
    " 145108,\n",
    " 145323,\n",
    " 145618,\n",
    " 146874,\n",
    " 146881,\n",
    " 147446,\n",
    " 147446,\n",
    " 147446,\n",
    " 147446,\n",
    " 147446,\n",
    " 147741,\n",
    " 148149,\n",
    " 148619,\n",
    " 149124,\n",
    " 153312,\n",
    " 155197,\n",
    " 158160,\n",
    " 159069,\n",
    " 160486,\n",
    " 160603,\n",
    " 160817,\n",
    " 161067,\n",
    " 165678,\n",
    " 168551,\n",
    " 168913,\n",
    " 169964,\n",
    " 170456,\n",
    " 171335,\n",
    " 171360,\n",
    " 171781,\n",
    " 171781,\n",
    " 175246,\n",
    " 176338,\n",
    " 176389,\n",
    " 176773,\n",
    " 176862,\n",
    " 177400,]\n",
    "\n",
    "len([k for k in lst if k<128000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set([len( datas[k]['context']) for k in range(len(datas))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zecheng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
